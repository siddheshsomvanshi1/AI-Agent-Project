# ğŸ³ Dockerizing Your LLaMA 3.2 AI Agent â€” Complete Guide

Containerize your Streamlit + Ollama + LLaMA 3.2 chatbot using Docker and Docker Compose.

---

## ğŸš¨ Very Important Understanding

You **CANNOT** run Ollama and Streamlit inside the same container properly in production.

**Best practice â€” two separate containers:**

```
Container 1 â†’ Ollama
Container 2 â†’ Streamlit App
```

**Why?**

- âœ… Separation of concerns
- âœ… Easier scaling
- âœ… Cleaner networking
- âœ… Production standard approach

**Tools used:** Docker + Docker Compose

---

## ğŸ—ï¸ Final Architecture

```
Browser
   â†“
Port 8501
   â†“
Streamlit Container
   â†“ (internal docker network)
Ollama Container (port 11434)
   â†“
llama3.2 model
```

---

## ğŸ”¹ STEP 1 â€” Install Docker on EC2

Update packages and install Docker:

```bash
sudo apt update
sudo apt install docker.io -y
```

Start and enable Docker:

```bash
sudo systemctl start docker
sudo systemctl enable docker
```

Add the `ubuntu` user to the docker group:

```bash
sudo usermod -aG docker ubuntu
```

> âš ï¸ You must logout and reconnect for this to take effect.

Exit the session:

```bash
exit
```

SSH back into your EC2 instance, then verify Docker is working:

```bash
docker --version
```

---

## ğŸ”¹ STEP 2 â€” Install Docker Compose

```bash
sudo apt install docker-compose -y
```

Verify installation:

```bash
docker-compose --version
```

---

## ğŸ”¹ STEP 3 â€” Modify app.py (IMPORTANT)

By default, the Ollama Python library connects to `localhost:11434`. Inside Docker, containers cannot reach each other via `localhost` â€” they must use the **container name** as the hostname.

### âœï¸ Edit app.py

**Replace:**

```python
import ollama
```

**With:**

```python
import ollama
import os

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
client = ollama.Client(host=OLLAMA_HOST)
```

**Then replace:**

```python
stream = ollama.chat(
```

**With:**

```python
stream = client.chat(
```

Save the file.

---

## ğŸ”¹ STEP 4 â€” Create Dockerfile

Inside your project folder, create the Dockerfile:

```bash
nano Dockerfile
```

Paste this exact content:

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

Save and exit.

---

## ğŸ”¹ STEP 5 â€” Create docker-compose.yml

In the same project folder, create the Compose file:

```bash
nano docker-compose.yml
```

Paste this exact content:

```yaml
version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  streamlit:
    build: .
    container_name: streamlit_app
    ports:
      - "8501:8501"
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:
```

Save and exit.

---

## ğŸ”¹ STEP 6 â€” Pull LLaMA Model Inside Container

> âš ï¸ This is a very important step. The model must be pulled inside the Ollama container.

First, start only the Ollama container:

```bash
docker-compose up -d ollama
```

Then pull the model inside the running container:

```bash
docker exec -it ollama ollama pull llama3.2:latest
```

Wait until the download is fully complete before moving to the next step.

---

## ğŸ”¹ STEP 7 â€” Build & Start Everything

Now build and start all containers:

```bash
docker-compose up -d --build
```

Check that both containers are running:

```bash
docker ps
```

You should see two containers running:

- `ollama`
- `streamlit_app`

---

## ğŸ”¹ STEP 8 â€” Open in Browser

Navigate to:

```
http://<EC2_PUBLIC_IP>:8501
```

ğŸ‰ **Your AI Agent is now running fully containerized!**

---

## ğŸ”¹ STEP 9 â€” Test Container Networking

Check the Streamlit container logs to confirm there are no connection errors:

```bash
docker logs streamlit_app
```

If everything is working correctly, you will see no connection errors in the output.

---

## ğŸ”¹ STEP 10 â€” Stop Containers

To stop all running containers:

```bash
docker-compose down
```

---

## ğŸ”¥ Production Improvements (Optional But Recommended)

### Auto Restart on Reboot

Already handled in `docker-compose.yml` with:

```yaml
restart: unless-stopped
```

This ensures your containers automatically restart if the EC2 instance reboots.

### Remove Old Images (Free Up Space)

```bash
docker system prune -a
```

---

## ğŸ’¡ Important Notes

### Model Persistence

Because the `docker-compose.yml` includes a named volume:

```yaml
volumes:
  - ollama_data:/root/.ollama
```

Your downloaded LLaMA model is **persisted** even if the container is stopped or restarted. You will not need to re-download it.

---

## ğŸ“ Final Project Structure

```
your-project/
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ lily.jpg
```
